<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
<link rel="stylesheet" href="../..//intel_styles.css" type="text/css" />
</head>
<body>
<div id="banner">
    <div id="bannerblock">
      <img src="../..//intel_logo.png" class="intellogo">
      <h1 class="title">Overview of OpenVINO&trade; toolkit Pre-trained Models</h1>
    </div>
  </div>
<div id="contentblock">
<h1 id="face-detection-adas-0001">face-detection-adas-0001</h1>
<h2 id="use-case-and-high-level-description">Use Case and High-Level Description</h2>
<p>Face detector for driver monitoring and similar scenarios. The network features a default MobileNet backbone that includes depth-wise convolutions to reduce the amount of computation for the 3x3 convolution block.</p>
<h2 id="example">Example</h2>
<div class="figure">
<img src="./face-detection-adas-0001.png" />

</div>
<h2 id="specification">Specification</h2>
<table>
<thead>
<tr class="header">
<th align="left">Metric</th>
<th align="left">Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">AP (head height &gt;10px)</td>
<td align="left">37.4%</td>
</tr>
<tr class="even">
<td align="left">AP (head height &gt;32px)</td>
<td align="left">84.8%</td>
</tr>
<tr class="odd">
<td align="left">AP (head height &gt;64px)</td>
<td align="left">93.1%</td>
</tr>
<tr class="even">
<td align="left">AP (head height &gt;100px)</td>
<td align="left">94.1%</td>
</tr>
<tr class="odd">
<td align="left">Min head size</td>
<td align="left">90x90 pixels on 1080p</td>
</tr>
<tr class="even">
<td align="left">GFlops</td>
<td align="left">2.8</td>
</tr>
<tr class="odd">
<td align="left">MParams</td>
<td align="left">1.1</td>
</tr>
<tr class="even">
<td align="left">Source framework</td>
<td align="left">Caffe*</td>
</tr>
</tbody>
</table>
<p>Average Precision (AP) is defined as an area under the <a href="https://en.wikipedia.org/wiki/Precision_and_recall">precision/recall</a> curve. Numbers are on <a href="http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/">Wider Face</a> validation subset.</p>
<h2 id="performance-fps">Performance (FPS)</h2>
<h3 id="configuration-1-ubuntu-16.04-intel-core-i5-6500-cpu-2.90ghz-fixed-gpu-gt2-1.00ghz-fixed-ddr4-pc170002133mhz">Configuration #1: Ubuntu* 16.04, Intel® Core™ i5-6500 CPU @ 2.90GHz fixed, GPU GT2 @ 1.00GHz fixed, DDR4 PC17000/2133MHz</h3>
<table style="width:100%;">
<colgroup>
<col width="14%" />
<col width="21%" />
<col width="25%" />
<col width="25%" />
<col width="12%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Caffe* CPU</th>
<th align="left">Inference Engine CPU</th>
<th align="left">Inference Engine GPU FP16</th>
<th align="left">Inference Engine GPU FP32</th>
<th align="left">OpenCV CPU</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">6.02</td>
<td align="left">87.07</td>
<td align="left">41.3</td>
<td align="left">34.06</td>
<td align="left">21.59</td>
</tr>
</tbody>
</table>
<h3 id="configuration-2-intel-movidius-myriad-2-ncsma2450-intel-core-i5-6500-cpu-2.90ghz-fixed">Configuration #2: Intel® Movidius™ Myriad™ 2 NCS/MA2450, Intel® Core™ i5-6500 CPU @ 2.90GHz fixed</h3>
<table>
<thead>
<tr class="header">
<th align="left">Inference Engine MYRIAD FP16</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">8.4</td>
</tr>
</tbody>
</table>
<h3 id="performance-disclaimer">Performance Disclaimer</h3>
<p>The benchmark results reported above may need to be revised as additional testing is conducted. The results depend on the specific platform configurations and workloads utilized in the testing, and may not be applicable to any particular user’s components, computer system or workloads. The results are not necessarily representative of other benchmarks and other benchmark results may show greater or lesser impact from mitigations.</p>
<p>Software and workloads used in performance tests may have been optimized for performance only on Intel microprocessors. Performance tests, such as SYSmark and MobileMark, are measured using specific computer systems, components, software, operations and functions. Any change to any of those factors may cause the results to vary. You should consult other information and performance tests to assist you in fully evaluating your contemplated purchases, including the performance of that product when combined with other products. For more complete information visit www.intel.com/benchmarks.</p>
<h3 id="optimization-notice">Optimization Notice</h3>
<p>Intel's compilers may or may not optimize to the same degree for non-Intel microprocessors for optimizations that are not unique to Intel microprocessors. These optimizations include SSE2, SSE3, and SSSE3 instruction sets and other optimizations. Intel does not guarantee the availability, functionality, or effectiveness of any optimization on microprocessors not manufactured by Intel. Microprocessor-dependent optimizations in this product are intended for use with Intel microprocessors. Certain optimizations not specific to Intel microarchitecture are reserved for Intel microprocessors. Please refer to the applicable product User and Reference Guides for more information regarding the specific instruction sets covered by this notice.</p>
<p>Notice revision #20110804</p>
<p>Notice revision #20110804</p>
<h2 id="inputs">Inputs</h2>
<ol style="list-style-type: decimal">
<li>name: &quot;input&quot; , shape: [1x3x384x672] - An input image in the format [BxCxHxW], where:
<ul>
<li>B - batch size</li>
<li>C - number of channels</li>
<li>H - image height</li>
<li>W - image width</li>
</ul></li>
</ol>
<p>Expected color order is BGR.</p>
<h2 id="outputs">Outputs</h2>
<ol style="list-style-type: decimal">
<li>The net outputs blob with shape: [1, 1, N, 7], where N is the number of detected bounding boxes. For each detection, the description has the format: [<code>image_id</code>, <code>label</code>, <code>conf</code>, <code>x_min</code>, <code>y_min</code>, <code>x_max</code>, <code>y_max</code>]
<ul>
<li><code>image_id</code> - ID of the image in the batch</li>
<li><code>label</code> - predicted class ID</li>
<li><code>conf</code> - confidence for the predicted class</li>
<li>(<code>x_min</code>, <code>y_min</code>) - coordinates of the top left bounding box corner</li>
<li>(<code>x_max</code>, <code>y_max</code>) - coordinates of the bottom right bounding box corner.</li>
</ul></li>
</ol>
<h2 id="legal-information">Legal Information</h2>
<p>Intel, the Intel logo, Intel Atom, Intel Core, Intel Xeon Phi, VTune and Xeon are trademarks of Intel Corporation in the U.S. and/or other countries.</p>
<p>*Other names and brands may be claimed as the property of others.</p>
<p>Copyright 2018 Intel Corporation.</p>
<p>This software and the related documents are Intel copyrighted materials, and your use of them is governed by the express license under which they were provided to you (License). Unless the License provides otherwise, you may not use, modify, copy, publish, distribute, disclose or transmit this software or the related documents without Intel's prior written permission.</p>
<p>This software and the related documents are provided as is, with no express or implied warranties, other than those that are expressly stated in the License.</p>
</div>
</body>
</html>
